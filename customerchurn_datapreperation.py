# -*- coding: utf-8 -*-
"""CustomerChurn-DataPreperation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QFrykCzNgxrcwQjoRaKyxiOQBLXT3QF0
"""

from google.colab import drive
drive.mount('/content/gdrive')

path = '/content/gdrive/MyDrive/Colab Notebooks/Files/CreditPrediction.csv'

import pandas as pd

data = pd.read_csv(path)
data.head()

data.info()

data.drop(['CLIENTNUM','Unnamed: 19'], axis=1, inplace=True)
data.head()

data.describe().round(2).T

# Define thresholds based on the given summary statistics
threshold_inactive_months = 3
threshold_transaction_count = 45
threshold_transaction_amount = 2155.50

# Define the 'Churn' column based on the thresholds
data['Churn'] = ((data['Months_Inactive_12_mon'] > threshold_inactive_months) |
                 (data['Total_Trans_Ct'] < threshold_transaction_count) |
                 (data['Total_Trans_Amt'] < threshold_transaction_amount)).astype(int)

# Check the distribution of the new target variable
print(data['Churn'].value_counts())

data.head()

observations_count = data.count()
missing_values = data.isnull().sum()
cardinality = data.nunique()

results = pd.DataFrame({
    'Observations_Count': observations_count,
    'Cardinality': cardinality,
    'Missing_Values': missing_values
})

print(results)

# Define a function to calculate multiple modes, their frequencies, and percentages
def calculate_multiple_mode_stats(column):
    mode_counts = data[column].value_counts()  # Count the occurrences of each unique value in the column
    modes = mode_counts.index.tolist()  # Get the list of modes (unique values sorted by frequency)
    frequencies = mode_counts.values.tolist()  # Get the list of frequencies corresponding to each mode
    percentages = [(freq / len(data)) * 100 for freq in frequencies]  # Calculate the percentage for each mode

    result = []  # Initialize an empty list to store the results
    for i in range(len(modes)):  # Loop through the modes
        result.append({
            'Mode': modes[i],  # Add the mode
            'Frequency': frequencies[i],  # Add the frequency of the mode
            'Percentage': percentages[i]  # Add the percentage of the mode
        })

    return result  # Return the result list

# Calculate for 'Gender'
Gender_stats = calculate_multiple_mode_stats('Gender')  # Calculate the stats for the 'Gender' column
print("Gender")
for stat in Gender_stats:  # Loop through the stats
    print(f"Mode: {stat['Mode']}, Frequency: {stat['Frequency']}, Percentage: {stat['Percentage']:.2f}%")  # Print the stats

# Calculate for 'Education_Level'
Education_Level_stats = calculate_multiple_mode_stats('Education_Level')  # Calculate the stats for the 'Education_Level' column
print("\nEducation_Level")
for stat in Education_Level_stats:  # Loop through the stats
    print(f"Mode: {stat['Mode']}, Frequency: {stat['Frequency']}, Percentage: {stat['Percentage']:.2f}%")  # Print the stats

# Calculate for 'Marital_Status'
Marital_Status_stats = calculate_multiple_mode_stats('Marital_Status')  # Calculate the stats for the 'Marital_Status' column
print("\nMarital_Status")
for stat in Marital_Status_stats:  # Loop through the stats
    print(f"Mode: {stat['Mode']}, Frequency: {stat['Frequency']}, Percentage: {stat['Percentage']:.2f}%")  # Print the stats

# Calculate for 'Card_Category'
Card_Category_stats = calculate_multiple_mode_stats('Card_Category')  # Calculate the stats for the 'Card_Category' column
print("\nCard_Category")
for stat in Card_Category_stats:  # Loop through the stats
    print(f"Mode: {stat['Mode']}, Frequency: {stat['Frequency']}, Percentage: {stat['Percentage']:.2f}%")  # Print the stats

import matplotlib.pyplot as plt

# Set up the figure
data.hist(bins=30, figsize=(15, 12), edgecolor='black')
plt.tight_layout()
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Convert the Churn column to categorical if it's not already
data['Churn'] = data['Churn'].astype('category')

# Set plot aesthetics
sns.set(style="whitegrid")

# Define categorical and numerical features
categorical_features = ['Gender', 'Education_Level', 'Marital_Status', 'Card_Category']
numerical_features = ['Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count',
                      'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit', 'Total_Revolving_Bal',
                      'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt', 'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1',
                      'Avg_Utilization_Ratio']


# Plot relationships between categorical features and Churn
for feature in categorical_features:
    plt.figure(figsize=(10, 5))
    sns.countplot(data=data, x=feature, hue='Churn')
    plt.title(f'Relationship between {feature} and Churn')
    plt.show()

# Plot relationships between numerical features and Churn
for feature in numerical_features:
    plt.figure(figsize=(10, 5))
    sns.boxplot(data=data, x='Churn', y=feature)
    plt.title(f'Relationship between {feature} and Churn')
    plt.show()

import numpy as np

# Identify and handle missing values
# Check for missing values
missing_values = data.isnull().sum()
print("Missing values before handling:\n", missing_values)

# Impute missing values for categorical columns with the mode
categorical_cols = ['Gender', 'Marital_Status', 'Card_Category']
for col in categorical_cols:
    data[col].fillna(data[col].mode()[0], inplace=True)

# Impute missing values for numerical columns with the median
numerical_cols = ['Months_on_book', 'Total_Relationship_Count']
for col in numerical_cols:
    data[col].fillna(data[col].median(), inplace=True)

# Check for duplicates and remove if any
data.drop_duplicates(inplace=True)

# Recheck for missing values
missing_values_after = data.isnull().sum()
print("Missing values after handling:\n", missing_values_after)

num_rows_after = data.shape[0]

# Function to remove outliers based on IQR (Reconsider the IQR Thresholds from 1.5 to 2 to cover more outliers)
def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 2 * IQR
    upper_bound = Q3 + 2 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Apply the function to each numeric column in the DataFrame
for column in data.select_dtypes(include=['float64', 'int64']).columns:
    data = remove_outliers_iqr(data, column)

# Get the number of rows in the dataset after removing outliers
num_rows_after = data.shape[0]

# Print the number of rows after removing outliers
print(f'The number of rows in the dataset after removing outliers is: {num_rows_after}')

# Define the grouping function
def categorize_education(level):
    higher_education = ['Doctorate', 'Graduate', 'Post-Graduate']
    if level in higher_education:
        return 'Higher Education'
    else:
        return 'No Higher Education'

# Apply the function to the Education_Level column to replace its values
data['Education_Level'] = data['Education_Level'].apply(categorize_education)

data.head()

# Convert categorical variables into numerical representations

# One-Hot Encoding for categorical features with multiple categories
categorical_features = ['Income_Category', 'Card_Category']
data = pd.get_dummies(data, columns=categorical_features, drop_first=True)

# Label Encoding for binary categorical features
binary_categorical_features = ['Gender', 'Marital_Status', 'Education_Level']
label_encoders = {}
# Added import for LabelEncoder
from sklearn.preprocessing import LabelEncoder

for col in binary_categorical_features:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

data.head()

# Calculate the correlation matrix between each of the input variable wioth the tarbget variable
corr_matrix = data.corr()

# Extract correlation with the target variable 'Churn'
corr_with_target = corr_matrix['Churn'].sort_values(ascending=False)

# Print the sorted correlations
print(corr_with_target)

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
corr_matrix = data.corr()
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix
corr_matrix = data.corr()

# Plot the heatmap
plt.figure(figsize=(18, 8))  # Optional: adjust the size of the heatmap
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix Heatmap")
plt.show()
plt.show()

# Normalize numerical features

# Identify numerical features
numerical_features = [
    'Customer_Age', 'Dependent_count', 'Months_on_book', 'Total_Relationship_Count',
    'Months_Inactive_12_mon', 'Contacts_Count_12_mon', 'Credit_Limit',
    'Total_Revolving_Bal', 'Total_Amt_Chng_Q4_Q1', 'Total_Trans_Amt',
    'Total_Trans_Ct', 'Total_Ct_Chng_Q4_Q1', 'Avg_Utilization_Ratio'
]

# Import the MinMaxScaler
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

data[numerical_features] = scaler.fit_transform(data[numerical_features])

# Convert boolean to int (0 or 1)
bool_columns = data.select_dtypes(include=['bool']).columns
data[bool_columns] = data[bool_columns].astype(int)

# Save the DataFrame to a new CSV file ready for machine learning algorithms
data.to_csv('processed_CreditPrediction.csv', index=False)

# Display the first few rows of the processed data
print(data.head())